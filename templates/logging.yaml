AWSTemplateFormatVersion: 2010-09-09
Description: CloudFormation Template to create S3 bucket and Kinesis processor for central logging.
Parameters:
  BucketName:
    Type: String
    Default: !Sub '${AWS::AccountID}-${AWS::Region}-central-logging'
    Description: Central logging bucket name
  LogS3Location:
    Type: String
    Default: GoPro/lambda-logs
    Description: >-
      S3 location for the logs streamed to this destination

Resources:
  CentralLoggingBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Ref BucketName
      LifecycleConfiguration:
        Rules:
          - Id: ArchiveToGlacier
            Prefix: ''
            Status: Enabled
            Transitions:
              - TransitionInDays: '60'
                StorageClass: GLACIER

  LoggingLambda:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: LoggingLambda
      Description: Firehose CloudWatch Log processing
      Handler: index.lambda_handler
      Role: !GetAtt 
        - LoggingLambdaRole
        - Arn
      Code:
        ZipFile: !Join 
          - |+

          - - import boto3
            - import json
            - import base64
            - import gzip
            - import StringIO
            - 'def transformLogEvent(log_event):'
            - '    return log_event['message']+"\n"'
            - 'def processRecords(records):'
            - '    for r in records:'
            - '        data = base64.b64decode(r['data'])
            - '        striodata = StringIO.StringIO(data)'
            - '    try:'
            - '        with gzip.GzipFile(fileobj=striodata, mode='r') as f:'
            - '            data = json.load(f.read())'
            - '    except IOError:'
            - '        pass'
            - '    recId = r['recordId']'
            - '    if type(data) == str:'
            - '        yield {'
            - '            "data": data,'
            - '            "result": "ok",'
            - '            "recordId": recId'
            - '        }'
            - '    elif data['messageType'] != "DATA_MESSAGE":'
            - '        yield {'
            - '            "result": "ProcessingFailed",'
            - '            "recordId": recId'
            - '        }'
            - '    else:'
            - '        data = "".join([transformLogEvent(e) for e in data['logEvents']])'
            - '        data = base64.b64encode(data'
            - '        yield {'
            - '            "data": data,'
            - '            "result": "ok",'
            - '            "recordId": recId'
            - '        }'
            - 'def putRecords(streamName, records, client, attemptsMade, maxAttempts):'
            - '    failedRecords = []'
            - '    codes = []'
            - '    errMsg = ""'
            - '    try:'
            - '        response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)'
            - '    except Exceptionm as e:'
            - '        failedRecords = records'
            - '        errMsg = str(e)'
            - '    if not failedRecords and response['FailedPutCount'] > 0:'
            - '        for idx, res in enumerate(response['RequestResponses']):'
            - '            if not res['ErrorsCode']:'
            - '                continue'
            - '            codes.append(res['ErrorCode'])'
            - '            failedRecords.append(records[idx])'
            - '        errMsg = "Individual error codes: "+"".join(codes)'
            - '    if len(failedRecords) > 0:'
            - '        if attemptsMade + 1 < maxAttempts:'
            - '            print("Retrying failed records")'
            - '            putRecords(streamName, failedRecords, client, attemptsMade+1, maxAttempts)'
            - '        else:'
            - '            raise RuntimeError("Could not put records after %s attempts." % (str(maxAttempts)))'
            - 'def handler(event, context):'
            - '    streamARN = event['deliveryStreamArn']'
            - '    region = streamARN.split(":")[3]'
            - '    streamName = StreamARN.split("/")[1]'
            - '    records = list(processRecords(event['records']))'
            - '    projectedSize = 0'
            - '    recordsToReingest = []'
            - '    for idx, rec in enumerate(records):'
            - '        if rec['results'] == "ProcessingFailed":'
            - '            continue'
            - '        projectedSize += len(rec['data'])+len(red['recordId'])'
            - '        if projectedSize > 4000000:'
            - '            recordsToReingest.append({'
            - '                "Data": rec['data']'
            - '            }]'
            - '            records[idx]['result'] = "Dropped"'
            - '            del(records[idx]['data'])'
            - '    if len(recordsToReingest) > 0:'
            - '        client = boto3.client('firehose', region_name=region)'
            - '        putRecords(streamName, recordsToReingest, client, attemptsMade=0, maxAttempts=20)'
            - '        print("Reingested %d records" % (len(recordsToReingest)))'
            - '    else:'
            - '        print("No records reingested")'
            - '    return {"records": records}'
      Runtime: python2.7
      MemorySize: '192'
      Timeout: '60'
    DependsOn: LoggingLambdaRole

  LoggingLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: LogginLambda-AssumeRole
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*

#Parameters:
#  LogBucketName:
#    Type: String
#    Default: central-log-do-not-delete
#    Description: Destination logging bucket
#  LogS3Location:
#    Type: String
#    Default: <BU>/<ENV>/<SOURCE_ACCOUNT>/<LOG_TYPE>/
#    Description: >-
#      S3 location for the logs streamed to this destination; example
#      marketing/prod/999999999999/flow-logs/
#  ProcessingLambdaARN:
#    Type: String
#    Default: ''
#    Description: CloudWatch logs data processing function
#  SourceAccount:
#    Type: String
#    Default: ''
#    Description: Source application account number

  LogStream:
    Type: 'AWS::Kinesis::Stream'
    Properties:
      Name: !Join 
        - ''
        - - !Ref 'AWS::StackName'
          - '-Stream'
      RetentionPeriodHours: 48
      ShardCount: 1
      Tags:
        - Key: Demo
          Value: CentralLogging
  LogRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - !Join 
                  - ''
                  - - logs.
                    - !Ref 'AWS::Region'
                    - .amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /service-role/
  LogRolePolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: !Join 
        - ''
        - - !Ref 'AWS::StackName'
          - '-LogPolicy'
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - 'kinesis:PutRecord'
            Resource:
              - !GetAtt 
                - LogStream
                - Arn
          - Effect: Allow
            Action:
              - 'iam:PassRole'
            Resource:
              - !GetAtt 
                - LogRole
                - Arn
      Roles:
        - !Ref LogRole
  LogDestination:
    Type: 'AWS::Logs::Destination'
    DependsOn:
      - LogStream
      - LogRole
      - LogRolePolicy
    Properties:
      DestinationName: !Join 
        - ''
        - - !Ref 'AWS::StackName'
          - '-Destination'
      RoleArn: !GetAtt 
        - LogRole
        - Arn
      TargetArn: !GetAtt 
        - MyStream
        - Arn
      DestinationPolicy: !Join 
        - ''
        - - '{"Version" : "2012-10-17","Statement" : [{"Effect" : "Allow",'
          - ' "Principal" : {"AWS" : "'
          - !Ref 'AWS::AccountId'
          - '"},'
          - '"Action" : "logs:PutSubscriptionFilter",'
          - ' "Resource" : "'
          - !Join 
            - ''
            - - 'arn:aws:logs:'
              - !Ref 'AWS::Region'
              - ':'
              - !Ref 'AWS::AccountId'
              - ':destination:'
              - !Ref 'AWS::StackName'
              - '-Destination'
          - '"}]}'
  S3deliveryStream:
    DependsOn:
      - S3deliveryRole
      - S3deliveryPolicy
    Type: 'AWS::KinesisFirehose::DeliveryStream'
    Properties:
      DeliveryStreamName: !Join 
        - ''
        - - !Ref 'AWS::StackName'
          - '-DeliveryStream'
      DeliveryStreamType: KinesisStreamAsSource
      KinesisStreamSourceConfiguration:
        KinesisStreamARN: !GetAtt 
          - LogStream
          - Arn
        RoleARN: !GetAtt 
          - S3deliveryRole
          - Arn
      ExtendedS3DestinationConfiguration:
        BucketARN: !Join 
          - ''
          - - 'arn:aws:s3:::'
            - !Ref BucketName
        BufferingHints:
          IntervalInSeconds: '60'
          SizeInMBs: '50'
        CompressionFormat: UNCOMPRESSED
        Prefix: !Ref LogS3Location
        RoleARN: !GetAtt 
          - S3deliveryRole
          - Arn
        ProcessingConfiguration:
          Enabled: 'true'
          Processors:
            - Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt
                    - LoggingLambda
                    - Arn
              Type: Lambda
  S3deliveryRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'
            Condition:
              StringEquals:
                'sts:ExternalId': !Ref 'AWS::AccountId'
  S3deliveryPolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: !Join 
        - ''
        - - !Ref 'AWS::StackName'
          - '-FirehosePolicy'
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - 's3:AbortMultipartUpload'
              - 's3:GetBucketLocation'
              - 's3:GetObject'
              - 's3:ListBucket'
              - 's3:ListBucketMultipartUploads'
              - 's3:PutObject'
            Resource:
              - !Join 
                - ''
                - - !Join 
                    - ''
                    - - 'arn:aws:s3:::'
                      - !Ref BucketName
              - !Join 
                - ''
                - - !Join 
                    - ''
                    - - 'arn:aws:s3:::'
                      - !Ref BucketName
                  - '*'
          - Effect: Allow
            Action:
              - 'lambda:InvokeFunction'
              - 'lambda:GetFunctionConfiguration'
              - 'logs:PutLogEvents'
              - 'kinesis:DescribeStream'
              - 'kinesis:GetShardIterator'
              - 'kinesis:GetRecords'
              - 'kms:Decrypt'
            Resource: '*'
      Roles:
        - !Ref S3deliveryRole

Outputs:
  CentralLogBucket:
    Description: Central log bucket
    Value: !Ref BucketName
    Export:
      Name: CentralLogBucketName
  LogginFunction:
    Description: Firehose CloudWatch Logging Function
    Value: !GetAtt
      - LoggingLambda
      - Arn
    Export: !Sub '${AWS::StackName}-Function'
  Destination:
    Description: Destination
    Value: !Join 
      - ''
      - - 'arn:aws:logs:'
        - !Ref 'AWS::Region'
        - ':'
        - !Ref 'AWS::AccountId'
        - ':destination:'
        - !Ref 'AWS::StackName'
        - '-Destination'
    Export:
      Name: !Sub '${AWS::StackName}-Destination'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Neural Network Codebook: 10 Epoch Training Example\n",
    "## Libraries, Global and Event Variables\n",
    "### Libraries\n",
    "The packages that will be needed by the Lambda Functions are declared in the `Utils` Python file, namely: \n",
    "- [datetime](https://docs.python.org/2/library/datetime.html) provides classes for manipulating dates and times in both simple and complex ways.\n",
    "- [numpy](http://www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test the model with unseen images at the end.\n",
    "- [boto3](https://pypi.python.org/pypi/boto3) is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.\n",
    "- [json](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript.\n",
    "- [os](https://docs.python.org/3/library/os.html) is a module the provides a portable way of using operating system dependent functionality. Particularly the  `environ` object is a mapping object representing the environment.\n",
    "- [uuid](https://docs.python.org/2/library/uuid.html#uuid.uuid4) creates a unique, random ID.\n",
    "- The [io](https://docs.python.org/2/library/io.html) module provides the Python interfaces to stream handling.\n",
    "- The Python interface to the [Redis](https://pypi.python.org/pypi/redis) key-value store.\n",
    "- [math](https://docs.python.org/3/library/math.html) to determine the last mini-batch size when partitioning the Training Data Set into mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import sys\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "import redis\n",
    "from redis import StrictRedis as redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Trigger Event\n",
    "To initiate the network training process, the dataset (**datasets.h5**) is uploaded to Amazon Simple Storage Services ([S3](https://aws.amazon.com/s3/)). This process triggers the S3 bucket event which starts the training process. A sample of the event payload sent to the SNN framework is as follows:\n",
    "\n",
    ">**Note:** In order for the *10 Epoch Sample* to work, please update the following lines in the code below and add the name of the S3 Bucket created during deployment. For example:\n",
    "```json\n",
    "    \"bucket\": {\n",
    "        \"arn\": \"arn:aws:s3:::<<BUCKET Name>>\",\n",
    "        \"name\": \"<<Bucket Name>>\",\n",
    "    ...\n",
    "```\n",
    "For this version of the implementation, the S3 Bucket is called **itsacat-demo** and the folder is called **training_input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::itsacat-demo\",\n",
    "                    \"name\": \"itsacat-demo\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "context = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish client connectivity to the various AWS services that the function will leverage, the following code creates the needed clients for the various AWS services, as global variables.\n",
    "\n",
    "### Global Variables\n",
    ">**Note:** The AWS Region is declared at the time of deployment. The `rgn` variable is declared from the above event in order to simulate its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\"\"\"\n",
    "Note: The Region is specifically declared for\n",
    "the 10 Epoch Sample\n",
    "\"\"\"\n",
    "rgn = 'us-east-1'\n",
    "s3_client = client('s3', region_name=rgn) # S3 access\n",
    "s3_resource = resource('s3')\n",
    "sns_client = client('sns', region_name=rgn) # SNS\n",
    "lambda_client = client('lambda', region_name=rgn) # Lambda invocations\n",
    "redis_client = client('elasticache', region_name=rgn) # ElastiCache\n",
    "# Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "# Set redis database to `10` as default for the main parameters\n",
    "cache = redis(host=endpoint, port=6379, db=10)\n",
    "dynamo_client = client('dynamodb', region_name=rgn)\n",
    "dynamo_resource = resource('dynamodb', region_name=rgn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Overview\n",
    "### Helper Functions\n",
    "All Helper funcitons are defined within the `Utils` Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T\n",
    "\n",
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255\n",
    "\n",
    "def initialize_data(parameters):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then initializes a random Weights and\n",
    "    Bias object for each layer.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- The initial/running parameters dictionary object\n",
    "    \n",
    "    Returns:\n",
    "    X -- Traing set features\n",
    "    Y - Training set labels\n",
    "    \"\"\"\n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "\n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "\n",
    "    # Reshape labels\n",
    "    Y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    X = standardize(train_set_x_orig)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def to_cache(db, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    db -- The ElastiCache database\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Width of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        print(\"The Object is not a supported serialization type\")\n",
    "        raise\n",
    "\n",
    "def from_cache(db, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    db -- ElastiCache database\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=db)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        sns_message = \"`from_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported de-serialization type\")\n",
    "        raise\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z.\n",
    "    \n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "    Return:\n",
    "    sigmoid(z)\n",
    "    \"\"\"\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function.\n",
    "    \n",
    "    Arguments:\n",
    "    z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    a -- Post-activation parameter, of the same shape as z\n",
    "    \"\"\"\n",
    "    a = np.maximum(0, z)\n",
    "    # Debug statement\n",
    "    #assert(a.shape == z.shape)\n",
    "    return a\n",
    "\n",
    "def sigmoid_backward(dA, z):\n",
    "    \"\"\"\n",
    "    Implement the derivative of the sigmoid function.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- Post-activation gradient, of any shape\n",
    "    z -- Cached linear activation from Forward prop\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the Cost with respect to z\n",
    "    \"\"\"\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    # Debug statement\n",
    "    #assert(dZ.shape == z.shape)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single ReLU unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- Post-activation gradient, of any shape\n",
    "    z -- Cached linear activation from Forward propagation\n",
    "    \n",
    "    Return:\n",
    "    dz -- Gradient of the Cost with respect to z\n",
    "    \"\"\"\n",
    "    dz = np.array(dA, copy=True) #converting dz to a correct object\n",
    "    # When z <= 0, set dz to 0 as well\n",
    "    dz[z <= 0] = 0\n",
    "    # Debug statement\n",
    "    #assert (dz.shape == z.shape)\n",
    "    return dz\n",
    "\n",
    "def random_minibatches(X, Y, batch_size=64):\n",
    "    \"\"\"\n",
    "    Creates a list of random smaller batches of X and Y\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Training examples input data of size (input size, no. of examples)\n",
    "    Y -- Training example labels vector of size (1, no. of examples)\n",
    "    batch_size -- Size of the mini batches\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_x, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    #np.random.seed(seed)\n",
    "    m = X.shape[1] # no. of training examples\n",
    "    mini_batches = [] \n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape(1, m)\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y), minus the end case.\n",
    "    num_complete_minibatches = math.floor(m / batch_size) # No. of mini batches\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * batch_size : (k + 1) * batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * batch_size : (k + 1) * batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Step 3: Deal with the end case\n",
    "    if m % batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def inv_counter(name, invID, task):\n",
    "    \"\"\"\n",
    "    Manages the Counter assigned to a unique Lambda Invocation ID, by\n",
    "    either setting it to 0, updating it to 1 or querying the value.\n",
    "   \n",
    "    Arguments:\n",
    "    name -- The Name of the function being invoked\n",
    "    invID -- The unique invocation ID created for the specific invocation\n",
    "    task -- Task to perform: set | get | update\n",
    "    \"\"\"\n",
    "    table = dynamo_resource.Table(name)\n",
    "    if task == 'set':\n",
    "        table.put_item(\n",
    "            Item={\n",
    "                'invID': invID,\n",
    "                'cnt': 0\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    elif task == 'get':\n",
    "        task_response = table.get_item(\n",
    "            Key={\n",
    "                'invID': invID\n",
    "            }\n",
    "        )\n",
    "        item = task_response['Item'] \n",
    "        return int(item['cnt'])\n",
    "        \n",
    "    elif task == 'update':\n",
    "        task_response = table.update_item(\n",
    "            Key={\n",
    "                'invID': invID\n",
    "            },\n",
    "            UpdateExpression='SET cnt = :val1',\n",
    "            ExpressionAttributeValues={\n",
    "                ':val1': 1\n",
    "            }\n",
    "        )\n",
    "\n",
    "def propogate(direction, batch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Determines the amount of \"hidden\" units based on the layer and loops\n",
    "    through launching the necessary `NeuronLambda` functions with the \n",
    "    appropriate state. Each `NeuronLambda` implements the cost function \n",
    "    OR the gradients depending on the direction.\n",
    "\n",
    "    Arguments:\n",
    "    direction -- The current direction of the propagation, either `forward` or `backward`.\n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "\n",
    "    Note: When launching NeuronLambda with multiple hidden unit,\n",
    "    remember to assign an ID, also remember to start at 1\n",
    "    and not 0. for example:\n",
    "    num_hidden_units = 5\n",
    "    for i in range(1, num_hidden_units + 1):\n",
    "        # Do stuff\n",
    "    \"\"\"\n",
    "    # Get the parameters for the layer\n",
    "    parameters = from_cache(db=batch, key=parameter_key)\n",
    "    num_hidden_units = parameters['neurons']['layer'+str(layer)]\n",
    "    \n",
    "    # Build the NeuronLambda payload\n",
    "    payload = {}\n",
    "    # Add the parameters to the payload\n",
    "    payload['state'] = direction\n",
    "    payload['batch'] = batch\n",
    "    payload['layer'] = layer\n",
    "    \n",
    "    # Determine process based on direction\n",
    "    if direction == 'forward':\n",
    "        # Launch Lambdas to propagate forward\n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this function's updates\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(db=batch, obj=parameters, name='parameters')\n",
    "\n",
    "        # Debug Statements\n",
    "        #print(\"Starting Forward Propagation for batch \" + str(batch) + \", layer \" + str(layer))\n",
    "\n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            \n",
    "            # Debug Statements\n",
    "            #print(\"Payload to be sent NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "                        \n",
    "            neuron_handler(event=payload, context=None)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    elif direction == 'backward':\n",
    "        # Launch Lambdas to propagate backward        \n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this functions updates\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(db=batch, obj=parameters, name='parameters')\n",
    "        \n",
    "        # Debug Statements\n",
    "        #print(\"Starting Backward Propagation for batch \" + str(batch) + \", layer \" + str(layer))\n",
    "\n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            \n",
    "            # Debug Statements\n",
    "            #print(\"Payload to be sent NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            neuron_handler(event=payload, context=None)\n",
    "            \n",
    "        return\n",
    "\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "def start_batch(batch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Starts a new mini-batch and configures the necessary\n",
    "    state tracking objects for the batch.\n",
    "    \n",
    "    Arguments:\n",
    "    batch -- Integer representing the \"current\" mini-batch\n",
    "    layer -- Integer representing the current hidden layer\n",
    "    \"\"\"\n",
    "    # Configure state parameters for this batch\n",
    "    parameters = from_cache(db=batch, key=parameter_key)\n",
    "    parameters['layer'] = layer\n",
    "    parameter_key = to_cache(\n",
    "        db=batch,\n",
    "        obj=parameters,\n",
    "        name='parameters'\n",
    "    )\n",
    "    \n",
    "    # Start Forward propagation\n",
    "    propogate(\n",
    "        direction='forward',\n",
    "        batch=batch,\n",
    "        layer=layer+1,\n",
    "        parameter_key=parameter_key\n",
    "    )\n",
    "\n",
    "def vectorizer(Outputs, Layer, batch, parameters):\n",
    "    \"\"\"\n",
    "    Creates a matrix of the individual neuron output for better vectorization.\n",
    "    \n",
    "    Arguments:\n",
    "    Outputs -- ElastiCache key to search for the data from `NeuronLambda`\n",
    "               e.g. 'a' for activations; 'dw' for Weight Derivatives\n",
    "    Layer -- Layer to search for neuron output that need to vectorized\n",
    "    batch -- The current mini-batch\n",
    "    parameters -- The current mini-batch parameters\n",
    "    \n",
    "    Returns:\n",
    "    result -- Matrix matching the size for the entire layer\n",
    "    \"\"\"\n",
    "    # Use the following Redis command to ensure a pure string is return for the key\n",
    "    r = redis(host=endpoint, port=6379, db=batch, charset=\"utf-8\", decode_responses=True)\n",
    "    search_results = []\n",
    "    # Compile a list of all the neurons in the search layer based on the search criteria\n",
    "    for n in range(1, parameters['neurons']['layer'+str(Layer)]+1):\n",
    "        tmp = r.keys('layer'+str(Layer)+'_'+str(Outputs)+'_'+str(n)+'|*')\n",
    "        search_results.append(tmp)\n",
    "    # Created an ordered list of neuron data keys\n",
    "    key_list = []\n",
    "    for result in search_results:\n",
    "        key_list.append(result[0])\n",
    "    # Create a dictionary of neuron data\n",
    "    Dict = {}\n",
    "    for data in key_list:\n",
    "        Dict[data] = from_cache(db=batch, key=data)\n",
    "    # Number of Neuron Activations for the search layer\n",
    "    num_neurons = parameters['neurons']['layer'+str(Layer)]\n",
    "    # Create a numpy array of the results, depending on the number\n",
    "    # of neurons (a Matrix of Activations)\n",
    "    matrix = np.array([arr.tolist() for arr in Dict.values()])\n",
    "    if num_neurons == 1:\n",
    "        # Single Neuron Activation\n",
    "        dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "        matrix = matrix.reshape(int(dims[0]), int(dims[1]))\n",
    "    else:\n",
    "        # Multiple Neuron Activations\n",
    "        matrix = np.squeeze(matrix)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def update_parameters_with_gd(W, b, dW, db, learning_rate, batch, layer, parameters):\n",
    "    \"\"\"\n",
    "    Updates parameters using one step of gradient descent for a layer.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- Matrix of the Weights for the layer\n",
    "    b -- Vector of the Bias for the layer\n",
    "    dW -- Matrix of the Derivatives of  Weights for the layer\n",
    "    db -- Vector of the Derivatives of the Bias for the layer\n",
    "    learning_rate -- Learning rate for the network, scalar\n",
    "    batch -- Current mini-batch\n",
    "    layer -- Current layer being optimized\n",
    "    \n",
    "    Returns:\n",
    "    parameter_key -- Updated parameter key with Weight and Bias data keys\n",
    "    \"\"\"\n",
    "    # Run gradient descent\n",
    "    W_prime = W - learning_rate * dW\n",
    "    b_prime = b - learning_rate * db\n",
    "    \n",
    "    # Update the current batch ElastiCache with Weights and Bias\n",
    "    parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "        db=batch,\n",
    "        obj=W_prime,\n",
    "        name='W'+str(layer+1)\n",
    "    )\n",
    "    parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "        db=batch,\n",
    "        obj=b_prime,\n",
    "        name='b'+str(layer+1)\n",
    "    )\n",
    "    \n",
    "    # Update parameters for the mini-batch\n",
    "    parameter_key = to_cache(\n",
    "        db=batch,\n",
    "        obj=parameters,\n",
    "        name='parameters'\n",
    "    )\n",
    "    \n",
    "    return parameter_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Handler Functions\n",
    "#### `launch_handler(event, context)`\n",
    "This `lambda_handler()` is triggered by the S3 event where training data is uploaded to S3. It further initializes the various components needed, such as:\n",
    "1. State tracking Objects:\n",
    "    - Overall Results (Cost) for each Epoch.\n",
    "    - Gradients for each layer.\n",
    "    - Initial and updated Weight parameter for each layer.\n",
    "    - Initial and updated Bias parameter for each layer.\n",
    "2. DynamoDB Storage:\n",
    "    - Invocation ID for each Lambda Function invocation to prevent duplicate invocation.\n",
    "    >**Note:** The DynamoDB Initialization is **NOT** recorded within the **Codebook**.\n",
    "3. Preprocessing the Input Data: \n",
    "    - Read in the initial *training*, *test* of Cat and Non-cat images.\n",
    "    - The function initially loads the data in `h5py` format and extracts the *training* and *test* data.\n",
    "    - The function further performs any standardization and normalization of the input data.\n",
    "    - The function also \"*flattens*\" the data into a column vector, thus performing **Vectorization**.\n",
    "    - This data is dumped to ElastiCache and will thus serve as **Layer 0** of the Neural Network.\n",
    "4. Initialize the mini-batches\n",
    "    - Initial shuffle and mini-batch partitioning of the pre-processed training data.\n",
    "5. Master Environment and State Tracking Variables:\n",
    "    - Loading the initial Neural Network Parameters (`parameters.json`) and augmenting these parameter with the state variables during the training process. The settings include overall parameters used by the `trainer` and `neuron` Lambda Functions, such as:\n",
    "        - Total number of epochs/iterations.\n",
    "        - Total number of batches.\n",
    "        - Total number of layers in the Neural Network (including the Output layer).\n",
    "        - Total number of \"neurons\" in each layer.\n",
    "        - The activation function to be used for each layer.  \n",
    "    - Initializing the **Hash Keys** for the various data sets in ElastiCache to be used by the subsequent functions to get access to the numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_handler(event, context):\n",
    "    # Determine if this is the initial launch of the function\n",
    "    if not event.get('state') == 'next':\n",
    "        # This is the first invocation, therefore set up the initial parameters\n",
    "        import shutil\n",
    "        source = './datasets/datasets.h5'\n",
    "        target = '/tmp/'\n",
    "        assert not os.path.isabs(source)\n",
    "        try:\n",
    "            shutil.copy(source, target)\n",
    "        except IOError as e:\n",
    "            print(\"Unable to copy local datasets file. %s\" % e)\n",
    "        \n",
    "        # Extract the neural network parameters from S3\n",
    "        input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "        dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "        settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "        try:\n",
    "            input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                print(sns_message)\n",
    "            else:\n",
    "                raise\n",
    "        with open('/tmp/parameters.json') as parameters_file:\n",
    "            parameters = json.load(parameters_file)\n",
    "\n",
    "        # Start building the Master parameters\n",
    "        # Input data set and parameter bucket\n",
    "        parameters['s3_bucket'] = event['Records'][0]['s3']['bucket']['name']\n",
    "        parameters['s3_key'] = event['Records'][0]['s3']['object']['key']\n",
    "        # Restrict codebook epochs to 10\n",
    "        parameters['epochs'] = 10\n",
    "        # Initial epoch for tracking\n",
    "        parameters['epoch'] = 0\n",
    "        # Initialize hash key tracking object\n",
    "        parameters['data_keys'] = {}\n",
    "        \n",
    "        # Initialize and pre-process the training set\n",
    "        X, Y = initialize_data(parameters)\n",
    "        \n",
    "        # Initialize the Weights and Bias using ReLU Initialization\n",
    "        # for the ReLU neurons\n",
    "        for l in range(1, parameters['layers']+1):\n",
    "            if l == 1:\n",
    "                \"\"\"\n",
    "                Note: This assumes Layer 1 uses the ReLU Activation.\n",
    "                \"\"\"\n",
    "                # Standard Wieght initialization for ReLU\n",
    "                W = np.random.randn(\n",
    "                    parameters['neurons']['layer'+str(l)],\n",
    "                    X.shape[0]\n",
    "                ) * np.sqrt((2.0 / X.shape[0]))\n",
    "            else:\n",
    "                if parameters['activations']['layer'+str(l)] == 'sigmoid':\n",
    "                    # Standard Weight initialization\n",
    "                    W = np.random.randn(\n",
    "                        parameters['neurons']['layer'+str(l)],\n",
    "                        parameters['neurons']['layer'+str(l-1)]\n",
    "                    ) / np.sqrt(parameters['neurons']['layer'+str(l-1)])\n",
    "                else:\n",
    "                    # Xavier Weight initialization for ReLu\n",
    "                    W = np.random.randn(\n",
    "                        parameters['neurons']['layer'+str(l)],\n",
    "                        parameters['neurons']['layer'+str(l-1)]\n",
    "                    ) * np.sqrt((2.0 / parameters['neurons']['layer'+str(l-1)]))\n",
    "            # Standard Bias initialization\n",
    "            b = np.zeros((parameters['neurons']['layer'+str(l)], 1))\n",
    "            # Upload the Weights and Bias to ElastiCache Master DB\n",
    "            parameters['data_keys']['W'+str(l)] = to_cache(db=15, obj=W, name='W'+str(l))\n",
    "            parameters['data_keys']['b'+str(l)] = to_cache(db=15, obj=b, name='b'+str(l))\n",
    "        \n",
    "        # Upload the pre-processed data sets to the Master ElastiCache DB (db=15)\n",
    "        # and update the parameters with the keys\n",
    "        parameters['data_keys']['X'] = to_cache(db=15, obj=X, name='X')\n",
    "        parameters['data_keys']['Y'] = to_cache(db=15, obj=Y, name='Y')\n",
    "        \n",
    "        # Initialize mini-batches\n",
    "        batch_size = parameters['batch_size']\n",
    "        batches = random_minibatches(X, Y, batch_size)\n",
    "        parameters['num_batches'] = len(batches)\n",
    "        \n",
    "        # Initialize DynamoDB table for tracking Costs\n",
    "        # Get the list of current DynamoDB Tables\n",
    "        current_tables = dynamo_client.list_tables()\n",
    "        if 'Costs' in current_tables['TableNames']:\n",
    "            # Delete the exisiting `Costs` table\n",
    "            dynamo_client.delete_table(TableName='Costs')\n",
    "            waiter = dynamo_client.get_waiter('table_not_exists')\n",
    "            waiter.wait(TableName='Costs')\n",
    "\n",
    "        # Create the \"fresh\" `Costs` table\n",
    "        table = dynamo_resource.create_table(\n",
    "            TableName='Costs',\n",
    "            KeySchema=[\n",
    "                {\n",
    "                    'AttributeName': 'epoch',\n",
    "                    'KeyType': 'HASH'\n",
    "                },\n",
    "            ],\n",
    "            AttributeDefinitions=[\n",
    "                {\n",
    "                    'AttributeName': 'epoch',\n",
    "                    'AttributeType': 'N'\n",
    "                },\n",
    "            ],\n",
    "            ProvisionedThroughput={\n",
    "                'ReadCapacityUnits': 5,\n",
    "                'WriteCapacityUnits': 5\n",
    "            }\n",
    "        )\n",
    "        table.meta.client.get_waiter('table_exists').wait(TableName='Costs')\n",
    "        \n",
    "        # Initialize the Results tracking object\n",
    "        results = {}\n",
    "        results['Start'] = str(datetime.datetime.now())\n",
    "        # Initialize tracking for first epoch\n",
    "        results['epoch0'] = {}\n",
    "        parameters['data_keys']['results'] = to_cache(\n",
    "            db=15, obj=results, name='results'\n",
    "        )\n",
    "        \n",
    "        # Finalize master parameters to ElastiCache\n",
    "        master_parameter_key = to_cache(\n",
    "            db=15, obj=parameters, name='parameters'\n",
    "        )\n",
    "                \n",
    "        # Configure batch specific parameters\n",
    "        current_batch = -1\n",
    "        for batch in batches:\n",
    "            # Create parameters that are specific to the batch, `batch_parameters`\n",
    "            current_batch += 1\n",
    "            (batch_X, batch_Y) = batch\n",
    "            m = batch_X.shape[1]\n",
    "            batch_parameters = parameters\n",
    "            batch_parameters['batch_ID'] = int(current_batch)\n",
    "            batch_parameters['data_keys']['A0'] = to_cache(\n",
    "                db=current_batch,\n",
    "                obj=batch_X,\n",
    "                name='A0'\n",
    "            )\n",
    "            batch_parameters['data_keys']['Y'] = to_cache(\n",
    "                db=current_batch,\n",
    "                obj=batch_Y,\n",
    "                name='Y'\n",
    "            )\n",
    "            batch_parameters['data_keys']['m'] = to_cache(\n",
    "                db=current_batch,\n",
    "                obj=m,\n",
    "                name='m'\n",
    "            )\n",
    "            \n",
    "            # Debug Statements\n",
    "            #print(\"\\n\"+\"\\n\"+\"Batch {} Parameters: \".format(current_batch))\n",
    "            #print(dumps(batch_parameters, indent=4, sort_keys=True))\n",
    "            \n",
    "            # Upload Batch parameters to respective ElastiCache database\n",
    "            batch_parameter_key = to_cache(\n",
    "                db=current_batch,\n",
    "                obj=batch_parameters,\n",
    "                name='parameters'\n",
    "            )\n",
    "            \n",
    "            # Confirm Batch key matches Master key\n",
    "            assert(str(batch_parameter_key) == str(master_parameter_key))\n",
    "            \n",
    "            # Initialize the payload for current batch to `TrainerLambda`\n",
    "            payload = {}\n",
    "            payload['state'] = 'start' # Initialize overall state\n",
    "            payload['batch'] = int(current_batch)\n",
    "            payload['parameter_key'] = master_parameter_key # Master parameter key\n",
    "            \n",
    "            # Launch `trainer_handler()` for the example.\n",
    "            trainer_handler(event=payload, context=None)\n",
    "        \n",
    "    else:        \n",
    "        # Determine current Epoch status\n",
    "        parameters = from_cache(db=15, key=event.get('parameter_key'))\n",
    "        epoch = parameters['epoch']\n",
    "        \n",
    "        if epoch == parameters['epochs'] - 1:\n",
    "            # This is the final epoch, therefore close out training\n",
    "            # and clean up.\n",
    "            \n",
    "            # Create dictionary of model parameters for prediction app\n",
    "            params = {}\n",
    "\n",
    "            # Consolidate the results from the parallel mini-batches.\n",
    "            # Step 1: Get the lowest Cost for EACH mini-batch from \n",
    "            # DynamoDB and determine which mini-batch has the lowest\n",
    "            # Error.\n",
    "            Costs = []\n",
    "            table = dynamo_resource.Table('Costs')\n",
    "            response = table.get_item(\n",
    "                Key={\n",
    "                    'epoch': epoch\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            item = response['Item']\n",
    "            # Get the cost for each mini-batch\n",
    "            for k, v in item.items():\n",
    "                if 'batch' in k:\n",
    "                    Costs.append(float(v))\n",
    "            # Calculate the index of the lowest Cost\n",
    "            best_batch = np.argmin(Costs)\n",
    "\n",
    "            # Step 2: Get the optimized Weights and Bias (by layer) from\n",
    "            # the mini-batch with the lowest error and upload these as\n",
    "            # the Weights and Bias for the prediction app.\n",
    "            for l in range(1, parameters['layers']+1):\n",
    "                # Get the weights from ElastiCache\n",
    "                W = from_cache(db=best_batch, key=parameters['data_keys']['W'+str(l)])\n",
    "                b = from_cache(db=best_batch, key=parameters['data_keys']['b'+str(l)])\n",
    "                # Update the model parameters for the prediction app\n",
    "                params['W'+str(l)] = W\n",
    "                params['b'+str(l)] = b\n",
    "            \n",
    "            # Create a model parameters file for use by prediction app\n",
    "            with h5py.File('/tmp/params.h5', 'w') as h5file:\n",
    "                for key in params:\n",
    "                    h5file['/' + key] = params[key]\n",
    "            # Upload model parameters file to S3\n",
    "            s3_resource.Object(\n",
    "                parameters['s3_bucket'],\n",
    "                'predict_input/params.h5'\n",
    "            ).put(Body=open('/tmp/params.h5', 'rb'))            \n",
    "            \n",
    "            # Debug Statements\n",
    "            print(\"Training Completed successfully!\\n\"+\"Final Cost = \"+dumps(Costs[best_batch]))\n",
    "            \n",
    "            # Update the final results with the average cost\n",
    "            final_results = from_cache(db=15, key=parameters['data_keys']['results'])\n",
    "            final_results['epoch'+str(epoch)]['cost'] = Costs[best_batch]\n",
    "            # Add the end time to the results\n",
    "            final_results['End'] = str(datetime.datetime.now())\n",
    "            # Upload the final results to S3\n",
    "            results_obj = s3_resource.Object(parameters['s3_bucket'],'training_results/results.json')\n",
    "            try:\n",
    "                results_obj.put(Body=json.dumps(final_results))\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                print(e)\n",
    "                raise\n",
    "        \n",
    "        else:\n",
    "            # This is not the final Epoch, therefore process the next\n",
    "            # epoch by consolidating the results from the parallel\n",
    "            # mini-batches.\n",
    "            # Step 1: Get the lowest Cost for EACH mini-batch form \n",
    "            # DynamoDB and determine which mini-batch has the lowest\n",
    "            # Error.\n",
    "            Costs = []\n",
    "            table = dynamo_resource.Table('Costs')\n",
    "            response = table.get_item(\n",
    "                Key={\n",
    "                    'epoch': epoch\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            item = response['Item']\n",
    "            # Get the cost for each mini-batch\n",
    "            for k, v in item.items():\n",
    "                if 'batch' in k:\n",
    "                    Costs.append(float(v))\n",
    "            # Calculate the index of the lowest Cost\n",
    "            best_batch = np.argmin(Costs)\n",
    "\n",
    "            # Adding temporary parameters placeholder in case current epoch\n",
    "            # produces the best Cost.\n",
    "            params = {}\n",
    "\n",
    "            # Step 2: Get the optimized Weights and Bias (by layer) from\n",
    "            # the mini-batch with the lowest error and upload these as\n",
    "            # the Weights and Bias for the predicton app.\n",
    "            for l in range(1, parameters['layers']+1):\n",
    "                # Get the weights from ElastiCache\n",
    "                W = from_cache(db=best_batch, key=parameters['data_keys']['W'+str(l)])\n",
    "                b = from_cache(db=best_batch, key=parameters['data_keys']['b'+str(l)])\n",
    "                # Update the model parameters for the pnext epoch\n",
    "                parameters['data_keys']['W'+str(l)] = to_cache(db=15, obj=W, name='W'+str(l))\n",
    "                parameters['data_keys']['b'+str(l)] = to_cache(db=15, obj=b, name='b'+str(l))\n",
    "\n",
    "                # Update params in case current epoch is below threshold\n",
    "                params['W'+str(l)] = W\n",
    "                params['b'+str(l)] = b\n",
    "\n",
    "            # Debug Statements\n",
    "            print(\"Cost after Epoch {} = {}\".format(epoch, Costs[best_batch]))\n",
    "            \n",
    "            # Update the results for this epoch with the average cost and\n",
    "            # send status updates for epochs every 100 epochs\n",
    "            update_results = from_cache(db=15, key=parameters['data_keys']['results'])\n",
    "            update_results['epoch'+str(epoch)]['cost'] = Costs[best_batch]\n",
    "            \n",
    "            # Close off current Epoch and move onto the next\n",
    "            # Initialize new mini-batches\n",
    "            batch_size = parameters['batch_size']\n",
    "            X, Y = initialize_data(parameters)\n",
    "            batches = random_minibatches(X, Y, batch_size)\n",
    "            parameters['num_batches'] = len(batches)\n",
    "            \n",
    "            # Initialize tracking for next epoch\n",
    "            epoch += 1\n",
    "            parameters['epoch'] = epoch\n",
    "            update_results['epoch'+str(epoch)] = {}\n",
    "            parameters['data_keys']['results'] = to_cache(\n",
    "                db=15, obj=update_results, name='results'\n",
    "            )\n",
    "            \n",
    "            # Finalize master parameters to ElastiCache\n",
    "            master_parameter_key = to_cache(\n",
    "                db=15, obj=parameters, name='parameters'\n",
    "            )\n",
    "            \n",
    "            # Configure batch specific parameters\n",
    "            current_batch = -1\n",
    "            for batch in batches:\n",
    "                # Create parameters that are specific to the batch, `batch_parameters`\n",
    "                current_batch += 1\n",
    "                (batch_X, batch_Y) = batch\n",
    "                m = batch_X.shape[1]\n",
    "                batch_parameters = parameters\n",
    "                batch_parameters['batch_ID'] = int(current_batch)\n",
    "                batch_parameters['data_keys']['A0'] = to_cache(\n",
    "                    db=current_batch,\n",
    "                    obj=batch_X,\n",
    "                    name='A0'\n",
    "                )\n",
    "                batch_parameters['data_keys']['Y'] = to_cache(\n",
    "                    db=current_batch,\n",
    "                    obj=batch_Y,\n",
    "                    name='Y'\n",
    "                )\n",
    "                batch_parameters['data_keys']['m'] = to_cache(\n",
    "                    db=current_batch,\n",
    "                    obj=m,\n",
    "                    name='m'\n",
    "                )\n",
    "\n",
    "                # Debug Statements\n",
    "                #print(\"\\n\"+\"\\n\"+\"Batch {} Parameters: \".format(current_batch))\n",
    "                #print(dumps(batch_parameters, indent=4, sort_keys=True))\n",
    "\n",
    "                # Upload Batch parameters to respective ElastiCache database\n",
    "                batch_parameter_key = to_cache(\n",
    "                    db=current_batch,\n",
    "                    obj=batch_parameters,\n",
    "                    name='parameters'\n",
    "                )\n",
    "\n",
    "                # Confirm Batch key matches Master key\n",
    "                assert(str(batch_parameter_key) == str(master_parameter_key))\n",
    "                \n",
    "                # Initialize the payload for current batch to `TrainerLambda`\n",
    "                payload = {}\n",
    "                payload['state'] = 'start' # Initialize overall state\n",
    "                payload['batch'] = int(current_batch)\n",
    "                payload['parameter_key'] = master_parameter_key # Master parameter key\n",
    "                \n",
    "                # Launch the `trainer_handler()` function for the 10 Epoch example.\n",
    "                trainer_handler(event=payload, context=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `trainer_handler(event, context)`\n",
    "This `lambda_handler()` function is the most critical function in the set in that it:\n",
    "1. Tracks and updates the state across the Mini-batches and the various layers of the Neural Network.\n",
    "2. Performs Vectorization on the Activation Row Vectors from each Neuron to create a *Matrix* of Activations.\n",
    "3. Launches the various Neurons (`NeuronLambda`) in each layer and tracks their output for *Forward* or *Backward* propagation.\n",
    "4. Calculates the *Cost* for each iteration of *Forward* propagation.\n",
    "5. Performs *Gradient Descent* for the current Mini-Batch.\n",
    "6. Calls the `LaunchLambda` to start the next Epoch.\n",
    "\n",
    "In order to accomplish this, the `TrainerLambda` has three possible states, `start`, `forward` and `backward`:\n",
    "1. `start`: This state starts the initial or subsequent training epochs and performs the following:\n",
    "    - Initializes the new weights and bias for the epoch.\n",
    "    - Updates the state table with these values.\n",
    "2. `forward`: This state processes the *forward* propagation step and launches the various hidden layer Neurons and supplies the necessary state information to these functions, such as:\n",
    "    - Input/Activation data location\n",
    "    - Weights and Bias.\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Activation Function for the Layer.\n",
    "3. `backward`: This state processes the *back* propagation step and launches the various hidden layer Neurons as well as supplies the necessary information for these functions, like:\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Current and previous Activations calculated from the forward propagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_handler(event, context):\n",
    "    \"\"\"\n",
    "    1. Processes the `event` variables from the various Lambda functions that call it, \n",
    "        i.e. `TrainerLambda` and `NeuronLambda`.\n",
    "    2. Determines the \"current\" state and then directs the next steps.\n",
    "    3. Performs Vectorization from the NeuronLambda forward propagation outputs.\n",
    "    4. Calculates the Cost.\n",
    "    5. Performs Gradient Descent given the gradients from the backward propagation outputs.\n",
    "    \"\"\"\n",
    "    # Get the current state from the invoking lambda function\n",
    "    state = event.get('state')\n",
    "    batch = event.get('batch')\n",
    "    layer = event.get('layer')\n",
    "    parameters = from_cache(db=batch, key=event.get('parameter_key'))\n",
    "    \n",
    "    # Execute appropriate action based on the current state\n",
    "    if state == 'forward':\n",
    "        # Get the Vectorized matrix of Activations from `NeuronLambda` output\n",
    "        A = vectorizer(Outputs='a', Layer=layer-1, batch=batch, parameters=parameters)\n",
    "        \n",
    "        # Add the `A` Matrix to `data_keys` for later Neuron use\n",
    "        A_name = 'A' + str(layer-1)\n",
    "        parameters['data_keys'][A_name] = to_cache(db=batch, obj=A, name=A_name)\n",
    "        \n",
    "        # Update ElastiCache with this function's data\n",
    "        parameter_key = to_cache(db=batch, obj=parameters, name='parameters')\n",
    "        \n",
    "        # Determine the location within Forwardprop\n",
    "        if layer > parameters['layers']:\n",
    "            # Location is at the end of forwardprop, therefore calculate Cost\n",
    "            # Get the training examples data and no. examples (`Y` and `m`)\n",
    "            Y = from_cache(db=batch, key=parameters['data_keys']['Y'])\n",
    "            m = from_cache(db=batch, key=parameters['data_keys']['m'])\n",
    "            \n",
    "            # Calculate the Cross-Entropy Cost\n",
    "            cost = (1. / m) * (-np.dot(Y, np.log(A).T) - np.dot(1 - Y, np.log(1 - A).T))\n",
    "            cost = np.squeeze(cost)\n",
    "            assert(cost.shape == ())\n",
    "            \n",
    "            # Add batch cost to DynamoDB Costs tracking object\n",
    "            table = dynamo_resource.Table('Costs')\n",
    "            table.update_item(\n",
    "                Key={\n",
    "                    'epoch': parameters['epoch']\n",
    "                },\n",
    "                UpdateExpression=\"Set #batch = :costval\",\n",
    "                ExpressionAttributeValues={\n",
    "                    ':costval': str(cost)\n",
    "                },\n",
    "                ExpressionAttributeNames={\n",
    "                    '#batch': 'batch'+str(batch)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            #print(\"Cost after Epoch {}, Batch {}: {}\".format(parameters['epoch'],batch,cost))\n",
    "            \n",
    "            # Initialize Backprop\n",
    "            # Calculate the derivative of the Cost with respect to the last\n",
    "            # activation and ensure that `Y` is the correct shape as the\n",
    "            # last activation.\n",
    "            Y = Y.reshape(A.shape)\n",
    "            dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "            dA_name = 'dA' + str(layer-1)\n",
    "            parameters['data_keys'][dA_name] = to_cache(db=batch, obj=dA, name=dA_name)\n",
    "            \n",
    "            # Update parameters from this function in ElastiCache\n",
    "            parameter_key = to_cache(db=batch, obj=parameters, name='parameters')\n",
    "            \n",
    "            # Start Backpropagation on NeuronLambda\n",
    "            propogate(direction='backward', batch=batch, layer=layer-1, parameter_key=parameter_key)\n",
    "            \n",
    "        else:\n",
    "            # Move to the next hidden layer for multiple layer networks\n",
    "            # Debug statement\n",
    "            #print(\"Propagating forward onto Layer \" + str(layer))\n",
    "            propogate(direction='forward', batch=batch, layer=layer, parameter_key=parameter_key)\n",
    "            \n",
    "    elif state == 'backward':\n",
    "        # Vectorize the derivatives\n",
    "        dZ = vectorizer(Outputs='dZ', Layer=layer+1, batch=batch, parameters=parameters)\n",
    "        \n",
    "        # Pre-process the derivatives of the Weights\n",
    "        dW = vectorizer(Outputs='dw', Layer=layer+1, batch=batch, parameters=parameters)\n",
    "        \n",
    "        # Pre-process the derivatives of the Bias\n",
    "        db = vectorizer(Outputs='db', Layer=layer+1, batch=batch, parameters=parameters)\n",
    "        db = vectorizer(Outputs='db', Layer=layer+1, batch=batch, parameters=parameters)\n",
    "        db = db.reshape(db.shape[0], 1)\n",
    "        \n",
    "        # Get the Learning rate\n",
    "        learning_rate = parameters['learning_rate']\n",
    "        \n",
    "        # Determine the location within Backprop\n",
    "        if batch == (parameters['num_batches'] - 1) and (layer == 0):\n",
    "            # Location is at the end of the final mini-batch, therefore\n",
    "            # get the necessary MASTER parameters for optimization and close\n",
    "            # out the parallel run by launching `LaunchLambda`.\n",
    "            W = from_cache(db=15, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(db=15, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            \n",
    "            # Update parameters for current layer to the mini-batch ElastiCache\n",
    "            parameter_key = update_parameters_with_gd(\n",
    "                W, b, dW, db,\n",
    "                learning_rate,\n",
    "                batch, layer,\n",
    "                parameters\n",
    "            )\n",
    "            \n",
    "            # Debug Statement\n",
    "            #print(\"Training complete for batch {}, epoch {}\".format(batch, parameters['epoch']))\n",
    "            \n",
    "            # Launch `LaunchLambda` to close out the mini-batches\n",
    "            # Initialize the payload for initial batch to `LaunchLambda`\n",
    "            payload = {}\n",
    "            payload['state'] = 'next' # Initialize overall state\n",
    "            payload['parameter_key'] = parameter_key\n",
    "            \n",
    "            # Launch `launch_handler()` function for 10 Epoch example.\n",
    "            launch_handler(event=payload, context=None)\n",
    "            \n",
    "        elif (batch < parameters['num_batches'] - 1) and (layer == 0):\n",
    "            # Location is at the end of the current mini-batch and\n",
    "            # backprop is finished, therefore get the necessary \n",
    "            # MASTER parameters for optimization and simply close\n",
    "            # out the mini-batch.\n",
    "            W = from_cache(db=15, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(db=15, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            \n",
    "            # Update parameters for current layer to the mini-bacth ElastiCache\n",
    "            parameter_key = update_parameters_with_gd(\n",
    "                W, b, dW, db,\n",
    "                learning_rate,\n",
    "                batch, layer,\n",
    "                parameters\n",
    "            )\n",
    "            # Debug Statement\n",
    "            #print(\"Training complete for batch {}, epoch {}\".format(batch, parameters['epoch']))\n",
    "                        \n",
    "        else:\n",
    "            # Location is still within the backprop process, therefore \n",
    "            # get the necessary MASTER parameters for optimization and \n",
    "            # continue backprop for the mini-batch.\n",
    "            W = from_cache(db=15, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(db=15, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            \n",
    "            # Calculate the current layer's activations with respect to the\n",
    "            # Cost/Error.\n",
    "            dA = np.dot(W.T, dZ)\n",
    "            dA_name = 'dA'+str(layer)\n",
    "            parameters['data_keys'][dA_name] = to_cache(\n",
    "                db=batch,\n",
    "                obj=dA,\n",
    "                name=dA_name\n",
    "            )\n",
    "            \n",
    "            # Update parameters for current layer to the mini-bacth ElastiCache\n",
    "            parameter_key = update_parameters_with_gd(\n",
    "                W, b, dW, db,\n",
    "                learning_rate,\n",
    "                batch, layer,\n",
    "                parameters\n",
    "            )\n",
    "            \n",
    "            # Move onto the the next hidden layer for multiple layer networks\n",
    "            propogate(direction='backward', batch=batch, layer=layer, parameter_key=parameter_key)\n",
    "            \n",
    "    elif state == 'start':\n",
    "        # Initialize the start of a mini-batch execution\n",
    "        layer = 0\n",
    "        start_batch(batch=batch, layer=layer, parameter_key=event.get('parameter_key'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `neuron_handler(event, context)`\n",
    "This `lambda_handler()` simulates a single *Perceptron* for both forward and backward propagation. If the state is `forward` then the function simulates forward propagation for $X$ to $Cost$ for the current layer. If the state is backward, then the function calculates the gradient of the derivative of the activation function for the current layer.\n",
    "\n",
    ">**Note:** This function also moves the state to the next or previous layer, depending on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_handler(event, context):\n",
    "    \"\"\"\n",
    "    This Lambda Function simulates a single Perceptron for both \n",
    "    forward and backward propagation.\n",
    "    \"\"\"    \n",
    "    # Get the current state\n",
    "    state = event.get('state')\n",
    "    batch = event.get('batch')\n",
    "    layer = event.get('layer')\n",
    "    # Get the ID of this individual neuron\n",
    "    ID = event.get('id')\n",
    "    # Determine is this is the last Neuron in the layer\n",
    "    last = event.get('last')\n",
    "    activation = event.get('activation')\n",
    "    \n",
    "    # Get the mini-batch parameters from Elasticache\n",
    "    parameters = from_cache(db=batch, key=event.get('parameter_key'))\n",
    "\n",
    "    # Debug Statement\n",
    "    #print(\"Starting {} propagation on Neuron: {}, for Batch {} and Layer {}\".format(state, str(ID), str(batch), str(layer)))\n",
    "    \n",
    "    # Forward propagation from A0 to Cost\n",
    "    if state == 'forward':\n",
    "        # Activations from the previous layer\n",
    "        A_prev = from_cache(db=batch, key=parameters['data_keys']['A'+str(layer - 1)])\n",
    "        # Get the MASTER weights for this neuron\n",
    "        w = from_cache(db=15, key=parameters['data_keys']['W'+str(layer)])[ID-1, :]\n",
    "        # Convert weights to a row vector\n",
    "        w = w.reshape(1, w.shape[0])\n",
    "        # Get the MASTER bias for this neuron as row vector\n",
    "        b = from_cache(db=15, key=parameters['data_keys']['b'+str(layer)])[ID-1, :]\n",
    "        # Perform the linear part of the layer's forward propagation\n",
    "        z = np.dot(w, A_prev) + b\n",
    "        # Upload the linear transformation results to batch ElastiCache for use with Backprop\n",
    "        to_cache(db=batch, obj=z, name='layer'+str(layer)+'_z_'+str(ID))\n",
    "        \n",
    "        # Perform non-linear activation based on the activation function\n",
    "        if activation == 'sigmoid':\n",
    "            a = sigmoid(z)\n",
    "        elif activation == 'relu':\n",
    "            a = relu(z)\n",
    "        else:\n",
    "            # No other activation functions supported at this time\n",
    "            pass\n",
    "        # Upload the results to batch ElastiCache for `TrainerLambda` to vectorize\n",
    "        to_cache(db=batch, obj=a, name='layer'+str(layer)+'_a_'+str(ID))\n",
    "\n",
    "        # Debug Statements\n",
    "        #print(\"Completed Forward Propgation for batch {}, layer {}.\".format(str(batch), str(layer)))\n",
    "        \n",
    "        if last == 'True':\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['layer'] = layer + 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(db=batch, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'forward'\n",
    "            payload['batch'] = batch\n",
    "            payload['layer'] = layer + 1\n",
    "            \n",
    "            # Debug Statement\n",
    "            #print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            # Launch `trainer_handler()` for 10 Epoch example.\n",
    "            trainer_handler(event=payload, context=None)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Backprop from Cost to X (A0)\n",
    "    elif state == 'backward':\n",
    "        # Get necessary parameters for the mini-batch\n",
    "        r = redis(host=endpoint, port=6379, db=batch, charset=\"utf-8\", decode_responses=True)\n",
    "        z_key = []\n",
    "        for z in r.scan_iter(match='layer'+str(layer)+'_z_'+str(ID)+'*'):\n",
    "            z_key.append(z)\n",
    "        z = from_cache(db=batch, key=z_key[0])\n",
    "        m = from_cache(db=batch, key=parameters['data_keys']['m'])\n",
    "        A_prev = from_cache(\n",
    "            db=batch,\n",
    "            key=parameters['data_keys']['A'+str(layer-1)]\n",
    "        )\n",
    "        \n",
    "        # Get the derivative of the current layer's activation,\n",
    "        # based on the size of the layer.\n",
    "        if layer == parameters['layers']:\n",
    "            # This is the last layer in the network, then assume\n",
    "            # a single neuron\n",
    "            dA = from_cache(db=batch, key=parameters['data_keys']['dA'+str(layer)])\n",
    "            # Get the MASTER weights for this layer\n",
    "            W = from_cache(db=15, key=parameters['data_keys']['W'+str(layer)])\n",
    "        else:\n",
    "            dA = from_cache(db=batch, key=parameters['data_keys']['dA'+str(layer)])[ID-1, :]\n",
    "            dA = dA.reshape(1, dA.shape[0])\n",
    "            # Get the MASTER weights for this layer\n",
    "            W = from_cache(db=15, key=parameters['data_keys']['W'+str(layer)])[ID-1, :]\n",
    "            W = W.reshape(1, W.shape[0])\n",
    "        \n",
    "        # Calculate the deivative of the activations\n",
    "        if activation == 'sigmoid':\n",
    "            dZ = sigmoid_backward(dA, z)\n",
    "        elif activation == 'relu':\n",
    "            dZ = relu_backward(dA, z)\n",
    "        else:\n",
    "            # No other activations supported at this time\n",
    "            pass\n",
    "        \n",
    "        # Upload the derivative of the activations for the \n",
    "        # mini-batch to ElastiCache, to be used by `TrainerLambda`.\n",
    "        to_cache(db=batch, obj=dZ, name='layer'+str(layer)+'_dZ_'+str(ID))\n",
    "        \n",
    "        # Calculate the derivative of the Weights for this neuron\n",
    "        dw = 1 / m * np.dot(dZ, A_prev.T)\n",
    "        # Upload the derivative of the weight for the \n",
    "        # neuron to ElastiCache, to be used by `TrainerLambda`.\n",
    "        to_cache(db=batch, obj=dw, name='layer'+str(layer)+'_dw_'+str(ID))\n",
    "        \n",
    "        # Debug statement\n",
    "        assert(dw.shape == W.shape)\n",
    "        \n",
    "        # Calculate the derivate of the bias for this neuron\n",
    "        db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        # Upload the derivative of the bias for the neuron\n",
    "        # to ElastiCache, to be used by `TrainerLambda`.\n",
    "        to_cache(db=batch, obj=db, name='layer'+str(layer)+'_db_'+str(ID))\n",
    "        \n",
    "        # Debug Statements\n",
    "        #print(\"Completed Back Propagation for batch {}, layer {}\".format(str(batch), str(layer)))\n",
    "        \n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['layer'] = layer - 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(db=batch, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'backward'\n",
    "            payload['batch'] = batch\n",
    "            payload['layer'] = layer - 1\n",
    "            \n",
    "            # Debug Statement\n",
    "            #print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            # Launch `trainer_handler()` function for 10 Epoch example.\n",
    "            trainer_handler(event=payload, context=None)\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sample Model Training\n",
    "### Overview\n",
    "The following demonstrates training the model over $10$ Epochs. The final results are stored on S3 and can be analyzed after training has been completed.\n",
    "\n",
    "### 1 - Trigger Event from S3\n",
    "**Simulate the training data being uploaded to S3 and Launching the training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after Epoch 0 = 0.6914032444285942\n",
      "Cost after Epoch 1 = 0.6883191381313808\n",
      "Cost after Epoch 2 = 0.6910632789480342\n",
      "Cost after Epoch 3 = 0.686973912309298\n",
      "Cost after Epoch 4 = 0.6894753991451259\n",
      "Cost after Epoch 5 = 0.6863783636718851\n",
      "Cost after Epoch 6 = 0.6804037520823715\n",
      "Cost after Epoch 7 = 0.6717221792922249\n",
      "Cost after Epoch 8 = 0.6608739108498947\n",
      "Training Completed successfully!\n",
      "Final Cost = 0.6458497635387419\n"
     ]
    }
   ],
   "source": [
    "# Simulate S3 event trigger data\n",
    "launch_handler(event, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - View the Training Results\n",
    "Although there are only $10$ iterations of the model training process, the Cost/Error can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processing time: 3 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d14b65c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the Results object stored on S3\n",
    "parameters = from_cache(db=15, key='parameters|json')\n",
    "bucket = parameters['s3_bucket']\n",
    "content = s3_resource.Object(bucket, 'training_results/results.json')\n",
    "file = content.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file)\n",
    "#print(json_content)\n",
    "costs = []\n",
    "for k, v in json_content.items():\n",
    "    # Get the cost at each epoch\n",
    "    if 'epoch' in k:\n",
    "        costs.append(v.get('cost'))\n",
    "    # Get the training start time\n",
    "    elif 'Start' in k:\n",
    "        start = datetime.datetime.strptime(v, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    # Get the training end time\n",
    "    else:\n",
    "        end = datetime.datetime.strptime(v, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "delta = end - start\n",
    "print(\"Total Processing time: {} minutes\".format(int(delta.total_seconds() / 60)))\n",
    "plt.plot(costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Learning Rate: \" + str(parameters['learning_rate']))\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: Full Model Training\n",
    "Now that model training process (and code) can be verified, it's time to train the model using the serverless framework **OR** view the un-optimized *Prediction API*. Refer back to the [**README**](../README.md) on the next steps."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

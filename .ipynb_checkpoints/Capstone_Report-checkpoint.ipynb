{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Trenton Potgieter\n",
    "January 31$^{st}$, 2017\n",
    "## I. Definition\n",
    "### Project Overview\n",
    "As one gets older, an increasingly difficult awareness of our parent's mortality becomes a serious concern. Personally, my parents are both in their early 70's and according to a study[^1] done in __2015__ by the __American Heart Association__, around __370,000__ people die of heart attacks each year and is the __No. 1__ cause of in the United States. In __2014__, around __356,500__ people experienced heart attacks out of the hospital. Of that  amount  only __12%__ survived due to emergency medical services intervention. Personally, I would not like my parents to be one the __88%__ who suffered from a fatal heart attack and didn't survive  due to the fact that there was no intervention by emergency medical services. According to the study, there is a prevalence of almost *third* of the population at risk of *Heart Disease* leading to a *Heart Attack* as one approaches __80+__ years of age. Having no personal experience in the Coronary Field of Medical research, it would be difficult for me to diagnose any potential warning signs, but with the advent of wearable technology, the mechanisms are in place to potentially aid in this early warning and detection of heart attacks. The majority of wearable technology today has the built-in ability to monitor heart rates. Therefore in this project, I proposed that this information can be uploaded or sent to a __data ingestion pipeline__ that this capable of interpreting, analyzing and detecting an the patterns that could be classified as symptoms of a heart attack. \n",
    "\n",
    "Additionally, since one of the potential symptoms is the increase in heart rates. There are a number of potential factors that influence the increase in heart rate, but there are well published guidelines[^2] that can be used to determine anomalous patterns. If these anomalies occur, the the __data ingestion pipeline__ could proactively determine if a heart attack is about to *or* has occurred and alert the appropriate emergency medical response. Thus proactively preventing a fatal or near-fatal heart attack. As am added benefit, the __pipeline__ mechanism can be used to monitor patients who are in *Cardiac Rehabilitation*[^3].\n",
    "\n",
    "### Problem Statement\n",
    "For this Project, I propose creating a classification pipeline that ingests heart-rate signal data (from a simulated wearable monitor) and classifies whether the subject is in a stressful situation that could lead to *Cardiac Unrest*. Additionally, in order to prevent a \"cry-wolf\" scenario or *false-positives*, the pipeline employs a consensus mechanism where three classifiers must all agree on the classification.\n",
    "\n",
    "![Figure 1: Training/Testing Pipeline](images/Pipeline.jpg)\n",
    "\n",
    "To address the scope of this project however, I propose training three separate supervised machine learning models by applying the following methodology to create the pipeline. Once created, the pipeline (see Figure 1.) will be used to test and deploy the models on a sample unseen data from the test subjects and hence predict their stress levels by following these steps:\n",
    "\n",
    "1. Collect already filtered PPG [^4] signal data with symbolic peaks (and other features) have been collected for a one-minute time segment. Each one-minute time segment is considered an observation labeled with the class `relax` or `stress`. Separate the input data into two separate repositories. One for the observations and one for the labeled output.\n",
    "2. Apply __Feature Extraction__ and if needed, __normalization__ and/or __standardization__ techniques to  pre-process the data.\n",
    "3. Define three separate models to evaluate the the data.\n",
    "    - Random Forest\n",
    "    - Support Vector Machine (SVM)\n",
    "    - Gaussian Naive Bayes\n",
    "4. Apply the models and measure their performance on a completely __separate__ and as yet __unseen__ dataset. This dataset is exactly the same as the training dataset except it is has no `State` label.\n",
    "\n",
    "The final classification is implementing a __Weighted Majority Rule Ensemble Classifier__[^5] based on the probability of the time segment observation belonging to either class, using the following:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\\\max_{i}\\sum^{m}_{j=1}w_{j}p_{ij},\n",
    "$$\n",
    "where $wj$ is the weight that can be assigned to the $j^{th}$ classifier.\n",
    "\n",
    "Once created, the pipeline, will be used to test and deploy the models on a sample unseen data from the new test subjects and hence predict their stress levels.\n",
    "\n",
    "### Metrics\n",
    "Since the success criteria of the project is based on the overall __probability__ of the time segment observation belonging to either class (`stressed` or `relaxed`), each individual model as well as the overall consensus pipeline will be evaluated using the following metrics:\n",
    "\n",
    "1. __Accuracy:__ $\\rightarrow$ The proportion of the total number of predictions that are \"correct\".\n",
    "2. __Recall:__ $\\rightarrow$ The measure of completeness of the classifier. In other words, if the label is `stressed`, how well does the model predict that the subject is `stressed`. Basically, the ratio of the number of observations the model can correctly recall, to the number of all correct observations.\n",
    "$$\n",
    "Recall = \\frac{True Positive}{True Positive + False Negative}\n",
    "$$\n",
    "\n",
    "3. __Precision:__ $\\rightarrow$ The number of positive predictions divided by the total positive class values. So, precision is the ratio of a number of observations the model can correctly predict to a number of all observations the model can recall. In other words, it is how precise the model's recall is.\n",
    "$$\n",
    "Precision = \\frac{True Positive}{True Positive + False Positive}\n",
    "$$\n",
    "\n",
    "4. __F1 Score:__ $\\rightarrow$ If the models are good at *Recall*, that doesn't necessarily mean that they are good at *Precision*. The *F1 Score* is the balanced average of the the two. This balanced *F1 Score* is necessary as an overall performance metric due to the fact that if there is a misclassification that the subject is under stress, but isn't, then the emergency medical services are called out unnecessarily. If however, there is a misclassification that the subject isn't stressed, but actually is, then this could result in a fatality. Having the *F1 Score* will allow us to allocate more weight to *Precision* or *Recall*.\n",
    "$$\n",
    "F1 \\ Score = \\frac{2 \\cdot Precision}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## II. Analysis\n",
    "### Data Exploration and Visualization\n",
    "The dataset used for this Project was obtained as part of a *Proof of Concept (POC)* project in the __Dell IoT Solutions Lab__ [^6] in Santa Clara, California, where a PPG [^4] Pulse sensor was used to measure Heart Rate Variability (HRV) [^7]  reading, similar to those found on current wearables like the __Fitbit Charge 2__ [^8]. The scope of the original POC is simply to verify if the data can be extracted and filtered to detect peaks in the PPG signal for a one minute data segment. Four separate test subjects (between the ages of 68 and 76) were subjected to different stimuli to induce *stress* and *relaxing* scenarios. The one minute observations (__300__ in total) are stored in a `data.csv` file. Each observation has __8__ specific features of the PPG waveform, namely:\n",
    "\n",
    "1. __Time__ $\\rightarrow$ Time Stamp of the observation.\n",
    "2. __AVRR__ $\\rightarrow$ Average \"normal\" hert beats.\n",
    "3. __AVHR__ $\\rightarrow$ Average total heart beats.\n",
    "4. __SDRR__ $\\rightarrow$ Standard Deviation of \"normal\" heart beats.\n",
    "5. __RMSRR__ $\\rightarrow$ Root Mean Squared of \"normal\" hear beats.\n",
    "6. __ppNN50__ $\\rightarrow$ Proportion of NN50 (50 successive \"normal\" heart beats) divided by total number of \"normal\" heart beats.\n",
    "7. __ppNN20__ $\\rightarrow$ Proportion of NN20 (20 successive \"normal\" heart beats) divided by total number of \"normal\" heart beats.\n",
    "8. __State__ $\\rightarrow$ \"Stressed\" or \"Relaxed\".\n",
    "\n",
    "The Table below shows a sample of the data of  the __8__ variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Environmental variables\n",
    "%matplotlib inline\n",
    "\n",
    "# Remove ALL warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/data.csv\", header = None)\n",
    "\n",
    "# Apply column names\n",
    "names = [\"Time\", \"AVHR\", \"AVRR\", \"SDRR\", \"RMSSD\", \"ppNN50\", \"ppNN20\", \"State\"]\n",
    "df.columns = names\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features above are themselves, descriptive statistics of the original PPG waveform, describing them individually does not provide significant value. Therefore analyzing the relationship between them provides better insight.\n",
    "\n",
    "For instance, from the scatter plot matrix (below), it can be seen that there is a linear correlation between __RMSSD__ and __SDRR__ as well as __RMSSD__ and __ppNN50__. However, the distribution of these features is __not__ evenly spread an skewed to the __right__, with a significant number of outliers (especially the __RMSSD__ variable). These factors could lead to high *bias* and low *variance* in the model. However there is a good spread and somewhat even distribution with __AVHR__, __AVRR__ and __ppNN20__ that may alleviate these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatterplot Matrix\n",
    "scatter_matrix(df, alpha=0.2, figsize=(16, 12), diagonal='kde')\n",
    "plt.suptitle(\"Scatter Plot Matrix of PPG waveform features\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three concerns with the dataset. The *first* is that fact that it has only __300__ observations, thus making it a relatively small data set. Based on this, the *second* is that we may not have an equal spread of labels. The *third* is the variables like __SDRR__ and __RMSSD__ show some extreme outliers and have different scales. \n",
    "\n",
    "To address the first two concerns, I propose leveraging __k-fold cross validation__. This process will be executed **10** times (10 Folds) where each fold is considered a unique set of training data. The advantage of this technique is that it can treat each test set uniquely, thus addressing the fact that the data set used is relatively small, and provide an average prediction result across the 10 folds. This process will be used for each of the three models. To ensure that there is not an __imbalance__ of labels, we can verify the count of each label (as shown below).__However__, the third concern will not be addressed since the data are themselves statistical measurement sof the original waveform. Scaling and normalizing the data to address outliers and scale may have a negative impact on the models capability to accurately predict the labels as it may overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot label counts\n",
    "df.State.value_counts().plot(kind = 'barh');\n",
    "\n",
    "# Display the label counts\n",
    "print df.State.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there is a somewhat equal spread of labels for `stress` (__157__) and `realax` (__143__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "### Benchmark\n",
    "Since there isn't another comparable methodology for the proposed pipeline and hence there isn't a comparable model implementation to serve as a benchmark, the pipeline methodology will be compared to a simple __Linear Classifier__. The evaluation criteria will be leveraged to compare each individual model's performance as well as the final ensemble model's performance against the *Linear Classifier's* baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv(\"data/data.csv\", header = None)\n",
    "\n",
    "# Apply column names\n",
    "names = [\"Time\", \"AVHR\", \"AVRR\", \"SDRR\", \"RMSSD\", \"ppNN50\", \"ppNN20\", \"State\"]\n",
    "df.columns = names\n",
    "df = df.drop(\"Time\", 1)\n",
    "\n",
    "# Convert the gatagorical column `State` into binary numbers\n",
    "le = preprocessing.LabelEncoder()\n",
    "df.State = le.fit_transform(df.State)\n",
    "\n",
    "# Create variables and labels\n",
    "X = df.drop(['State'], 1)\n",
    "y = df['State']\n",
    "\n",
    "# Create final results\n",
    "results = pd.DataFrame({\"Metric:\": [\"Accuracy\", \"Recall\", \"Precision\", \"F1 Score\"]})\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the Model\n",
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *Logistic Regression* baseline, the data was separated into __10__ *Folds* of *Testing* and *Training* data to avoid over fitting and average out the final accuracy across multiple trials since the data set are small. Additionally, since the `Time` variable has no impact on the model, it was removed. \n",
    "\n",
    "The only preprocessing step involved performing __Feature Extraction__. For more details, see the __Data Preprocessing__ section. The baseline scores are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(model, X, y, cv = 10, scoring = \"accuracy\")\n",
    "precision = cross_val_score(model, X, y, cv = 10, scoring =\"precision\")\n",
    "recall = cross_val_score(model, X, y, cv = 10, scoring = \"recall\")\n",
    "f1 = cross_val_score(model, X, y, cv = 10, scoring = \"f1\")\n",
    "\n",
    "tmp_df = pd.DataFrame({\"benchmark\": [np.mean(accuracy),\\\n",
    "                                     np.mean(precision),\\\n",
    "                                     np.mean(recall),\\\n",
    "                                     np.mean(f1)]})\n",
    "print \"Accuracy: {}\".format(np.mean(accuracy))\n",
    "print \"Recall: {}\".format(np.mean(recall))\n",
    "print \"Precision: {}\".format(np.mean(precision))\n",
    "print \"F1 Score: {}\".format(np.mean(f1))\n",
    "results = results.join(tmp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<!--- Original code with 20 Fold cross validation\n",
    "accuracy = cross_val_score(model, X, y, cv = 20, scoring = \"accuracy\")\n",
    "precision = cross_val_score(model, X, y, cv = 20, scoring =\"precision\")\n",
    "recall = cross_val_score(model, X, y, cv = 20, scoring = \"recall\")\n",
    "f1 = cross_val_score(model, X, y, cv = 20, scoring = \"f1\")\n",
    "\n",
    "print \"Accuracy: {}\".format(np.mean(accuracy))\n",
    "print \"Recall: {}\".format(np.mean(recall))\n",
    "print \"Precision: {}\".format(np.mean(precision))\n",
    "print \"F1 Score: {}\".format(np.mean(f1))\n",
    "--->\n",
    "\n",
    "<!--- Original code with NO cross validation\n",
    "# Check the accuracy on the entire data set\n",
    "model.score(X, y)\n",
    "--->\n",
    "\n",
    "The initial baseline benchmark for the *Linear Classifier* is a __F1 Score__ of 75.4% accuracy, which is a fairly decent score considering no optimization or ensemble learning has been performed. The objective of **Section III** below is to improve on this metric.\n",
    "\n",
    "---\n",
    "\n",
    "## III. Methodology\n",
    "### Data Preprocessing\n",
    "The *first* step in preprocessing the data for model fitting, is to perform __Feature Extraction__. *Feature Extraction* separates the incoming signal data from the heart rate monitor into two separate training data sets. The first data set are the signal observations, while the second data set are the training labels associated with each observation. The *second* step is to further convert the labels into a binary integer value, demarcating: \n",
    "\n",
    "- ${0} \\rightarrow$ __ `relax`__\n",
    "- ${1} \\rightarrow$ __`stress`__\n",
    "\n",
    "<!--- Original preprocessing verbage\n",
    "The *third* step is to __Standardize__ and __Normalize__ the data, using the following formulas:\n",
    ">**Standardize:**\n",
    "$$\n",
    "X = \\frac{\\sum^{n}_{i=1}(x_{i} - \\mu)}{\\sigma}\n",
    "$$\n",
    "\n",
    ">**Normalize:**\n",
    "$$\n",
    "X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "Doing this, will \"center\" the distribution of the data to zero, with a standard deviation of one (creating a normal distribution) and sets scale of each observation to be between $0$ and $1$. By applying these techniques, we eliminate the variability of the data so that that each observation's scale is on par with the other and thus the model is not biased toward stronger variables. \n",
    "--->\n",
    "\n",
    "<!--- Original code for standardization and normalization\n",
    "# Import preprocessing libaries\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardize the data\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "df_std = std_scale.transform(X)\n",
    "\n",
    "# Scale the data\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(X)\n",
    "df_minmax = minmax_scale.transform(X)\n",
    "\n",
    "# Print min and max values for AVHR as an example\n",
    "print \"AVHR Minimum Scaled Value: {}\".format(df_minmax[:, 0].min())\n",
    "print \"AVHR Maximum Scaled Value: {}\".format(df_minmax[:, 0].max())\n",
    "--->\n",
    "\n",
    "<!---\n",
    "The example above, shows the *minimum* and *maximum* values of the now preprocessed __AVHR__ variable.\n",
    "--->\n",
    "\n",
    "<!-- Original Code\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "clf1 = RandomForestClassifier()\n",
    "clf2 = SVC(C = 1.0, gamma = \"auto\", kernel = \"rbf\", random_state = 42, verbose = False, probability = True)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "print('20-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3], [\"Random Forest\", \"SVM\", \"Naive Bayes\"]):\n",
    "\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv = 20, scoring = \"accuracy\")\n",
    "    print \"{} Accuracy: {}\".format(label, scores.mean())\n",
    "--->\n",
    "\n",
    "### Implementation\n",
    "Once the data has been preprocessed, the three chosen classification models can be applied:\n",
    "\n",
    "#### Random Forest\n",
    "The __Random Forest__[^9] classifier (an ensemble method itself) is used to cluster points of data in functional groups. When the data set is large and/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group. Based on this, this classifier is selected as one of the methods due to the fact that the data set are relatively small. \n",
    "\n",
    "To perform the classification, the algorithm clusters the data in groups and subgroups, or decision trees. At each split of the tree, variables are chosen at random as to whether the data points have a close relationship or not.\n",
    "The algorithm makes multiple trees to create a \"forest\", with each tree being different due to the fact that the decision split occurs on different variables. The classification is used to predict which tree in the forests makes the best classification of the label data.\n",
    "\n",
    "#### Support Vector Machine (SVM)\n",
    "The __SVM__[^10] is a classifier that tries to draw the *best* line to separate the classifications, in this case `stress` and `relax`. Since there can be multiple decision boundaries to correctly separate the two classes, this model is implemented to find the best separation that maximizes the distance between all the data point in each classification. \n",
    "\n",
    "#### Gaussian Naive Bayes\n",
    "Since the overall objective for the final pipeline and *weighted majority rule* classifier is based on probability of an observation belonging to one one of the two classes, an obvious choice for the third classification method is the __Gaussian Naive Bayes__[^11] classifier as it is a simple probabilistic classifier based on applying *Bayes' theorem* with strong (or naive) independence assumptions between the features. In other words, a *Naive Bayes* classifier considers each feature to contribute independently to the final probability, regardless of any possible correlations between the features.\n",
    "\n",
    "For additional optimization techniques used with the three classifiers, see the next section, **Refinement**. To see the resultant evaluation metrics results and how they compare with the *baseline* results, see **Section IV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute the 3 models and display their score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(42)\n",
    "\n",
    "clf1 = RandomForestClassifier(criterion = \"entropy\", max_depth = 4, min_samples_split = 4,\\\n",
    "                             min_samples_leaf = 5, max_features = \"sqrt\", random_state = 42)\n",
    "clf2 = SVC(C = 1.0, gamma = \"auto\", kernel = \"rbf\", random_state = 42, verbose = False, probability = True)\n",
    "clf3 = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "To refine the the above, a number of model specific hyper parameters have been used to improve the overall performance from the default parameters. As a whole, the fact that the dataset is significantly small becomes apparent in that to get a better, more accurate fit, the original idea for a __10-Fold Cross Validation__ has to be increased to __20 Folds__. The following highlights the individual hyper parameters tweaked for each model:\n",
    "\n",
    "#### Random Forest Classifier\n",
    "For the *Random Forest* classifier, the following hyper parameters are changed:\n",
    "- __criterion__ $\\rightarrow$ The default setting to measure the quality of a split on a tree is the *Gini* impurity. Since *Gini* is intended for continuous attributes and *Entropy* is intended for attributes that occur in classes as in this case, `entropy` is used for the *criterion* parameter.[^12]\n",
    "- __max_depth__ $\\rightarrow$ The maximum depth of each tree (vertical depth). This setting has been set to `4` as a higher depth will allow the model to learn relations that a specific to a particular data point and hence contribute to overfitting.\n",
    "- __min_samples_split__ $\\rightarrow$ The minimum amount of observations required in a node before it can be considered for splitting. This setting has been set to `4` as higher values prevent a model from learning relations that are too specific to a particular \"tree\".\n",
    "- __min_samples_leaf__ $\\rightarrow$ The minimum amount of observations required in a terminal node (leaf node). This setting has been set to `5` which is one value higher then *min_samples_split* as this too controls overfitting.\n",
    "- __max_features__ $\\rightarrow$ The number of features to consider while searching for the best split. This is set to `0.7` (or 70%), to avoid overfitting as the parameter is cross-validated with a 70/30 split.\n",
    "\n",
    "#### Support Vector Machine (SVM)\n",
    "For the *SVM* classifier, the following hyper parameter are changed:\n",
    "- __kernel__ $\\rightarrow$\n",
    "- __gamma__ $\\rightarrow$\n",
    "- __C__ $\\rightarrow$\n",
    "- __probability__ $\\rightarrow$\n",
    "\n",
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin as Classifiers\n",
    "import operator\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, Classifiers):\n",
    "    \n",
    "    def __init__(self, clfs, weights=None):\n",
    "        self.clfs = clfs\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.clfs:\n",
    "            clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.classes_ = np.asarray([clf.predict(X) for clf in self.clfs])\n",
    "        if self.weights:\n",
    "            avg = self.predict_proba(X)\n",
    "\n",
    "            maj = np.apply_along_axis(lambda x: max(enumerate(x), key=operator.itemgetter(1))[0], axis=1, arr=avg)\n",
    "\n",
    "        else:\n",
    "            maj = np.asarray([np.argmax(np.bincount(self.classes_[:,c])) for c in range(self.classes_.shape[1])])\n",
    "\n",
    "        return maj\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.probas_ = [clf.predict_proba(X) for clf in self.clfs]\n",
    "        avg = np.average(self.probas_, axis=0, weights=self.weights)\n",
    "\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Rule Ensemble Classifier\n",
    "\n",
    "```\n",
    "    Ensemble classifier for scikit-learn estimators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    clf : `iterable`\n",
    "      A list of scikit-learn classifier objects.\n",
    "    weights : `list` (default: `None`)\n",
    "      If `None`, the majority rule voting will be applied to the predicted class labels.\n",
    "        If a list of weights (`float` or `int`) is provided, the averaged raw probabilities (via `predict_proba`)\n",
    "        will be used to determine the most confident class label.\n",
    "\n",
    "```\n",
    "    \n",
    "```\n",
    "        Fit the scikit-learn estimators.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "            Training data\n",
    "        y : list or numpy array, shape = [n_samples]\n",
    "            Class labels\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "        maj : list or numpy array, shape = [n_samples]\n",
    "            Predicted class labels by majority rule\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : numpy array, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "        avg : list or numpy array, shape = [n_samples, n_probabilities]\n",
    "            Weighted average probability for each class per sample.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "### Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"20-fold cross validation:\\n\"\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3], [\"Random Forest\", \"SVM\", \"Naive Bayes\"]):\n",
    "\n",
    "    accuracy = cross_val_score(clf, X, y, cv = 20, scoring = \"accuracy\")\n",
    "    precision = cross_val_score(clf, X, y, cv=20, scoring='precision')\n",
    "    recall = cross_val_score(clf, X, y, cv=20, scoring='recall')\n",
    "    f1 = cross_val_score(clf, X, y, cv=20, scoring='f1')\n",
    "\n",
    "#    tmp_df = pd.DataFrame({label: [np.mean(accuracy),\\\n",
    "#                                     np.mean(precision),\\\n",
    "#                                     np.mean(recall),\\\n",
    "#                                     np.mean(f1)]})\n",
    "    print \"Scores for {}\\n\".format(label)\n",
    "    print \"Accuracy: {}\".format(np.mean(accuracy))\n",
    "    print \"Recall: {}\".format(np.mean(recall))\n",
    "    print \"Precision: {}\".format(np.mean(precision))\n",
    "    print \"F1 Score: {}\".format(np.mean(f1))\n",
    "    print \"-----------------------------------------\\n\"\n",
    "#    results = results.join(tmp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- Original code without additional scores\n",
    "np.random.seed(42)\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], [\"Random Forest\", \"SVM\", \"Naive Bayes\", \"Ensemble\"]):\n",
    "\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=20, scoring='accuracy')\n",
    "    print \"Accuracy: {} {} {}\".format(scores.mean(), scores.std(), label)\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], [\"Random Forest\", \"SVM\", \"Naive Bayes\", \"Ensemble\"]):\n",
    "\n",
    "    accuracy = cross_val_score(clf, X, y, cv=20)\n",
    "    precision = cross_val_score(clf, X, y, cv=20, scoring='precision')\n",
    "    recall = cross_val_score(clf, X, y, cv=20, scoring='recall')\n",
    "    f1 = cross_val_score(clf, X, y, cv=20, scoring='f1')\n",
    "    \n",
    "    print \"Scores for {}\\n\".format(label)\n",
    "    print \"Accuracy: {}\".format(np.mean(accuracy))\n",
    "    print \"Recall: {}\".format(np.mean(recall))\n",
    "    print \"Precision: {}\".format(np.mean(precision))\n",
    "    print \"F1 Score: {}\".format(np.mean(f1))\n",
    "    print \"-----------------------------------------\\n\"\n",
    "\n",
    "# To be run on the final pass of the notebook    \n",
    "#    tmp_df = pd.DataFrame({label: [np.mean(accuracy),\\\n",
    "#                                   np.mean(precision),\\\n",
    "#                                   np.mean(recall),\\\n",
    "#                                   np.mean(f1)]})\n",
    "#    results = results.join(tmp_df)\n",
    "#results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "### Free-Form Validation\n",
    "\n",
    "### Reflection\n",
    "\n",
    "### Improvement\n",
    "\n",
    "#### Weight Refinement\n",
    "\n",
    "<!--- Original code to enhance the model by finding the best weights\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "\n",
    "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "\n",
    "            eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[w1,w2,w3])\n",
    "            scores = cross_validation.cross_val_score(\n",
    "                                            estimator=eclf,\n",
    "                                            X=X,\n",
    "                                            y=y,\n",
    "                                            cv=20,\n",
    "                                            scoring='accuracy',\n",
    "                                            n_jobs=1)\n",
    "\n",
    "            df.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n",
    "            i += 1\n",
    "\n",
    "df.sort(columns=['mean', 'std'], ascending=False).head(3)\n",
    "--->\n",
    "\n",
    "<!--- Original code to apply the weights from above\n",
    "np.random.seed(42)\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[2.0,1.0,3.0])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], [\"Random Forest\", \"SVM\", \"Naive Bayes\", \"Ensemble\"]):\n",
    "\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=20, scoring='accuracy')\n",
    "    print \"Accuracy: {} {} {}\".format(scores.mean(), scores.std(), label)\n",
    "--->\n",
    "\n",
    "~~It is the objective of this project to re-apply the resulting pipeline to a set of new test subjects and hopefully provide a viable prototype that can preemptively warn of potential heart attacks. Based on this final classification, additional future actions can be implemented that are currently outside the scope of this project.~~\n",
    "\n",
    "\n",
    "\n",
    "## VI. References\n",
    "\n",
    "[^1]: (https://www.heart.org/idc/groups/ahamah-public/@wcm/@sop/@smd/documents/downloadable/ucm_480086.pdf)\n",
    "[^2]: (http://www.heart.org/HEARTORG/HealthyLiving/PhysicalActivity/FitnessBasics/Target-Heart-Rates_UCM_434341_Article.jsp#.WHEiXbGZNE4)\n",
    "[^3]: (https://www.nhlbi.nih.gov/health/health-topics/topics/rehab)\n",
    "[^4]: (https://en.wikipedia.org/wiki/Photoplethysmogram)\n",
    "[^5]: (http://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting)\n",
    "[^6]: (https://www.dell.com/en-us/work/learn/internet-of-things-labs)\n",
    "[^7]: (http://www.myithlete.com/what-is-hrv/)\n",
    "[^8]: (https://www.fitbit.com/charge2)\n",
    "[^9]: (https://en.wikipedia.org/wiki/Random_forest)\n",
    "[^10]: (https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "[^11]: (https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "[^12]: (http://paginas.fe.up.pt/~ec/files_1011/week%2008%20-%20Decision%20Trees.pdf)\n",
    "\n",
    "<!---\n",
    "# Convert 'stress' and 'relax' to binary\n",
    "one_hot = pd.get_dummies(df['State'])\n",
    "data = df.drop('State', 1)\n",
    "data = data.join(one_hot)\n",
    "\n",
    "# Create variables and labels\n",
    "y = data[['stress', 'relax']]\n",
    "X = data.drop(['stress', 'relax' ], 1)\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
